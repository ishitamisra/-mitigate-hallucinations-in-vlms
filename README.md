# -mitigate-hallucinations-in-vlms
 Combining the visual contrastive decoding with steering vector intervention to mitigate object hallucinations in Large Vision-Language Models. 

The task at hand is to develop an object hallucination-reduction model integrating Visual Contrastive Decoding (VCD) and Activation Steering Decoding (ASD) with LLaVA 1.5 as the base vision-language model. The foundation of the technical architecture begins with a hallucination detector to measure LLaVA's raw hallucination rate. COCO's 80 object vocabulary was imported to evaluate how many objects LLaVA observed but weren't included in the ground truth captions. Next, the VCD implementation followed by Leng et al. (2023) essentially utilizes Gaussian blur to emphasize features that appear in both the original and blurred visual inputs. To achieve this, I ran dual forward passes via PyTorch tensors. I fed the clear image and the prompt to obtain orig_logits and blurred image and prompt to obtain blur_logits. Following the contrastive formula found in [1], I computed adjusted_logits = orig_logits + α × (orig_logits - blur_logits). Hyperparameter α (contrast strength) was set to 0.1. β (blur intensity) = 0.5, defining a 5-pixel blur radius. These parameters were adopted from [1], but it is possible that additional experimentation of α and β may yield better performance. Hence, I implemented the get_vcd_logits() method that processes both the original and blurred images, computing contrastive logits to reduce statistical bias and language priors. 
The second key technique implemented involves defining a steering vector from intermediate hidden states. Following [2]'s approach, I first defined the compute_steering_vector() method which takes two collections of hidden state vectors – the first is generated from examples where LLaVA's output is aligned with ground truth descriptions and the second from hallucinated outputs. To form these 2 collections, I processed 1000 COCO validation images, extracted the final token representation from (hidden_states[-1][0, -1]), and used HallucinationDetector()'s object-level evaluation to compute hallucination scores. If the computed score was ≤ 0.3, the respective hidden state was added to truth_vectors and scores ≥ 0.7 were added to halluc_vectors. (These score thresholds were empirically found in [2] and weren't experimented upon). I then stacked these vectors into tensors to compute truth_mean - halluc_mean, and performed L2 normalization to account for varying hidden state activations. This ultimately defines a steering direction that points toward the truth direction and away from the hallucination direction in the model's activation space. Since ASD is a real-time intervention, I integrated PyTorch hooks to modify the activations of layer 16 during the forward pass. asd_steering_bias() applies the steering equation z_steered = z + λv, where each λ = 0.2 is applied to the positive steering vector and λ = 0.4 to the negative steering vector. These values for λ were optimally set as per [2]. Contrast decoding is then applied via the equation asd_logits = (1 + self.alpha) * positive_logits - self.alpha * negative_logits, essentially steering away token predictions that are more hallucination-prone. (self.alpha is another hyperparameter that I set to 1.0, followed in [2]). After bi-directional steering is completed, I unregistered the forward hooks via remove_hook() in order to prevent progressive over-steering of the activation layers.The next step was combining both VCD and ASD outputs. The generate_with_vcd_and_steering() method creates a cascaded pipeline where the resulting logits produced from ASD are processed through VCD using vcd_logits = outputs.logits + α × (outputs.logits - blur_outputs.logits). Hence, two key steps are performed for every forward pass now: ASD steers the hidden states towards truth and VCD emphasizes the visual ground truth within the token output predictions. 
To perform evaluation, each output was scored by extracting COCO objects from both generated text and ground truth captions to compute individual hallucination rates as len(hallucinated_objects) / len(mentioned_objects). This evaluation method  was applied across two phases: first during the baseline VCD-only generation phase for collecting steering vector representations, and then again during the combined VCD+ASD evaluation phase. I then averaged the individual hallucination scores across the 1000 COCO images, which yielded an average hallucination rate of 14% for the combined approach compared to the baseline performance. This means that on average, only 14% of objects mentioned in the generated output were not present in the ground truth captions, which translates to an 86% accuracy of the VCD + ASD model performance.
*Key Challenges:
Obtaining and configuring the LLaVA model from Hugging Face proved more complex than anticipated due to version dependency conflicts. This was due to the fact that changes in newer transformers library versions (4.40+) made AutoProcessor API incompatible with LLaVA 1.5's expected input formatting. I also ran into CUDA runtime errors because of older PyTorch versions. To resolve these compatibility issues, I consulted Haotian Liu's LLaVA GitHub repository which specified the tested configuration I needed to use along with the compatible versions of both the accelerate and bitsandbytes libraries.Another significant challenge I faced was understanding the concept behind steering vectors and real-time intervention in neural networks. The idea that you could intercept activations at specific layers and nudge them toward desired behaviors was not one that I had come across before – This required extensive additional research into PyTorch's hook mechanism, understanding how forward passes work in transformer architectures, and learning about activation patching techniques. I spent considerable time studying the ASD paper's methodology and experimenting with simple hook implementations to understand how z_steered = z + λv could meaningfully alter model behavior without breaking the learned representations. I realized that steering vectors essentially create a "bias" in the representation space that shifts the model's predictions towards truthful patterns. Another major challenge was understanding how to integrate VCD's logit-level predictions with ASD's activation-level steering, since these represented fundamentally different intervention points in the generation pipeline. Initially, I wasn't sure whether to apply VCD first then ASD, or vice versa. I then understood that VCD operates on the final output logits by taking the difference between both original versus blurred visual inputs, while ASD operates on intermediate hidden states by steering internal representations. A more cascaded approach needed to be integrated, where ASD steers the internal representations toward truth during the forward pass and then VCD would filter the resulting logits to therefore emphasize features in original visual inputs.
[1] Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding. https://arxiv.org/abs/2311.16922
[2] Activation Steering Decoding: Mitigating Hallucination in Large Vision-Language Models through Bidirectional Hidden State Intervention. https://openreview.net/pdf?id=XfvmkVvnCq 
