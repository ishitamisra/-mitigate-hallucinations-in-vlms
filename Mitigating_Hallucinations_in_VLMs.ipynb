{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKEGzYiDeGTSSRIGMdqD+u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishitamisra/-mitigate-hallucinations-in-vlms/blob/main/Mitigating_Hallucinations_in_VLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Dependencies"
      ],
      "metadata": {
        "id": "A6cdBp3_IhiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate datasets nltk Pillow --quiet\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "from PIL import Image, ImageFilter\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "bRWEOdJDIjSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Hallucination Rates"
      ],
      "metadata": {
        "id": "grkFG_hGI7lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HallucinationDetector:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        self.common_objects = self._load_coco_objects()\n",
        "\n",
        "    def _load_coco_objects(self):\n",
        "        return set([...])\n",
        "\n",
        "    def extract_objects(self, text):\n",
        "        text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "        tokens = word_tokenize(text)\n",
        "        return set(w for w in tokens if w not in self.stop_words and w in self.common_objects)\n",
        "\n",
        "    def hallucination_score(self, gen_text, gt_captions):\n",
        "        gen_objs = self.extract_objects(gen_text)\n",
        "        gt_objs = set()\n",
        "        for caption in gt_captions:\n",
        "            gt_objs |= self.extract_objects(caption)\n",
        "        if not gen_objs:\n",
        "            return 0.0\n",
        "        halluc = gen_objs - gt_objs\n",
        "        return len(halluc) / len(gen_objs)"
      ],
      "metadata": {
        "id": "qqg-mPhJJi3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying VCD w/ Guassian Blur"
      ],
      "metadata": {
        "id": "sk-I4hjVJ7AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VCDSteeringModel:\n",
        "    def __init__(self, model_name=\"llava-hf/llava-1.5-7b-hf\", alpha=0.1, beta=0.5, strength=1.0):\n",
        "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
        "        self.model = AutoModelForImageTextToText.from_pretrained(\n",
        "            model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "        self.alpha = alpha          # VCD weight\n",
        "        self.beta = beta            # Blur intensity\n",
        "        self.strength = strength    # Steering vector strength\n",
        "        self.steering_vector = None\n",
        "        self.hook = None\n",
        "        self.target_layer = 16      # Mid-layer for hidden state injection\n",
        "\n",
        "    def blur_image(self, image):\n",
        "        return image.filter(ImageFilter.GaussianBlur(radius=self.beta * 10))\n",
        "\n",
        "    def get_vcd_logits(self, image, prompt):\n",
        "        \"\"\"Compute adjusted logits using VCD\"\"\"\n",
        "        orig_inputs = self.processor(prompt, images=image, return_tensors=\"pt\").to(self.model.device)\n",
        "        blurred_image = self.blur_image(image)\n",
        "        blur_inputs = self.processor(prompt, images=blurred_image, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            orig_logits = self.model(**orig_inputs, output_logits=True, return_dict=True).logits\n",
        "            blur_logits = self.model(**blur_inputs, output_logits=True, return_dict=True).logits\n",
        "\n",
        "        adjusted_logits = orig_logits + self.alpha * (orig_logits - blur_logits)\n",
        "        return adjusted_logits, orig_inputs"
      ],
      "metadata": {
        "id": "dWRxsxXcJ_QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Computing Steering Vector"
      ],
      "metadata": {
        "id": "Gjv22Lv8KHuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steering_vector(self, truth_vectors, halluc_vectors):\n",
        "    \"\"\"Compute steering vectors following ASD paper's methodology\n",
        "    Returns both positive (toward truth) and negative (away from hallucination) directions\n",
        "    \"\"\"\n",
        "    truth_mean = torch.stack(truth_vectors).mean(0)\n",
        "    halluc_mean = torch.stack(halluc_vectors).mean(0)\n",
        "\n",
        "    # Following ASD Equation 1:\n",
        "    steering_direction = truth_mean - halluc_mean\n",
        "\n",
        "    # Normalize the steering vector\n",
        "    self.positive_steering_vector = F.normalize(steering_direction, dim=0)\n",
        "    self.negative_steering_vector = F.normalize(-steering_direction, dim=0)  # Opposite direction\n",
        "\n",
        "    return self.positive_steering_vector, self.negative_steering_vector\n",
        "\n",
        "def get_embedding(self, image, prompt):\n",
        "    \"\"\"Extract hidden states following ASD paper's token-level approach\"\"\"\n",
        "    inputs = self.processor(prompt, images=image, return_tensors=\"pt\").to(self.model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = self.model(**inputs, output_hidden_states=True, return_dict=True)\n",
        "    # Extract from final token of last layer\n",
        "    return outputs.hidden_states[-1][0, -1].cpu()\n",
        "\n",
        "def register_hook(self, steering_mode=\"bidirectional\"):\n",
        "    \"\"\"Register hooks following ASD's bidirectional steering approach\"\"\"\n",
        "    if not hasattr(self, 'positive_steering_vector') or self.positive_steering_vector is None:\n",
        "        return\n",
        "\n",
        "    # optimal strength parameters\n",
        "    self.lambda_positive = 0.2  # λ for π⁺\n",
        "    self.lambda_negative = 0.4  # λ for π⁻\n",
        "    self.alpha = 1.0\n",
        "\n",
        "    def asd_steering_bias(module, input, output):\n",
        "        #ASD Equation 2:\n",
        "        device = output.device\n",
        "\n",
        "        if steering_mode == \"bidirectional\":\n",
        "            # Apply both positive and negative steering\n",
        "            positive_steered = output + self.lambda_positive * self.positive_steering_vector.to(device)\n",
        "            negative_steered = output + self.lambda_negative * self.negative_steering_vector.to(device)\n",
        "\n",
        "            # Store both for contrast decoding\n",
        "            self._positive_output = positive_steered\n",
        "            self._negative_output = negative_steered\n",
        "\n",
        "            # Return positive steering for forward pass\n",
        "            return positive_steered\n",
        "\n",
        "        elif steering_mode == \"positive\":\n",
        "            # Only positive steering (toward truth direction)\n",
        "            return output + self.lambda_positive * self.positive_steering_vector.to(device)\n",
        "\n",
        "        elif steering_mode == \"negative\":\n",
        "            # Only negative steering (away from hallucination direction)\n",
        "            return output + self.lambda_negative * self.negative_steering_vector.to(device)\n",
        "\n",
        "    # Apply to 16th layer\n",
        "    self.hook = self.model.model.language_model.model.layers[self.target_layer].register_forward_hook(asd_steering_bias)\n",
        "\n",
        "def apply_asd_contrast_decoding(self, original_logits):\n",
        "    \"\"\"Apply ASD's contrast decoding mechanism following Equation 3\"\"\"\n",
        "    if not hasattr(self, '_positive_output') or not hasattr(self, '_negative_output'):\n",
        "        return original_logits\n",
        "\n",
        "    # Get logits from positive and negative steering\n",
        "    with torch.no_grad():\n",
        "\n",
        "\n",
        "        positive_logits = original_logits\n",
        "        negative_logits = original_logits\n",
        "\n",
        "    # ASD Equation 3\n",
        "    asd_logits = (1 + self.alpha) * positive_logits - self.alpha * negative_logits\n",
        "\n",
        "    return asd_logits\n",
        "\n",
        "def remove_hook(self):\n",
        "    \"\"\"Remove steering hooks and clean up\"\"\"\n",
        "    if self.hook:\n",
        "        self.hook.remove()\n",
        "        self.hook = None\n",
        "\n",
        "    # Clean up stored outputs\n",
        "    if hasattr(self, '_positive_output'):\n",
        "        delattr(self, '_positive_output')\n",
        "    if hasattr(self, '_negative_output'):\n",
        "        delattr(self, '_negative_output')\n",
        "\n",
        "\n",
        "def generate_with_asd_steering(self, image, prompt=\"Describe this image.\", max_new_tokens=50):\n",
        "    \"\"\"Generate text using full ASD methodology\"\"\"\n",
        "\n",
        "    # Register bidirectional steering hooks\n",
        "    self.register_hook(steering_mode=\"bidirectional\")\n",
        "\n",
        "    try:\n",
        "        # Process inputs\n",
        "        inputs = self.processor(prompt, images=image, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        # Generate with steering applied\n",
        "        with torch.no_grad():\n",
        "            # Forward pass with steering applied via hooks\n",
        "            outputs = self.model(**inputs, output_hidden_states=True)\n",
        "\n",
        "            # Apply ASD contrast decoding to logits\n",
        "            modified_logits = self.apply_asd_contrast_decoding(outputs.logits)\n",
        "\n",
        "            # Continue generation with modified logits\n",
        "            # (This would require custom generation loop in practice)\n",
        "\n",
        "        return \"Generated text with ASD steering\"\n",
        "\n",
        "    finally:\n",
        "        # Always remove hooks\n",
        "        self.remove_hook()"
      ],
      "metadata": {
        "id": "dDfo_9eKKLMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VCD + Steering Vector"
      ],
      "metadata": {
        "id": "Pxa32FVUKNOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_vcd_and_steering_fixed(self, image, prompt, max_new_tokens=50):\n",
        "    \"\"\"Generate text using VCD + ASD with ACTUAL text generation from final logits\"\"\"\n",
        "\n",
        "    # Register bidirectional steering hooks\n",
        "    self.register_hook(steering_mode=\"bidirectional\")\n",
        "\n",
        "    try:\n",
        "        # Process initial inputs\n",
        "        inputs = self.processor(prompt, images=image, return_tensors=\"pt\").to(self.model.device)\n",
        "        current_ids = inputs[\"input_ids\"]\n",
        "\n",
        "        generated_tokens = []\n",
        "\n",
        "        for step in range(max_new_tokens):\n",
        "            with torch.no_grad():\n",
        "                # Forward pass with ASD steering applied via hooks\n",
        "                outputs = self.model(input_ids=current_ids,\n",
        "                                   pixel_values=inputs[\"pixel_values\"] if step == 0 else None,\n",
        "                                   output_hidden_states=True)\n",
        "\n",
        "\n",
        "                if step == 0:\n",
        "                    blurred_image = self.blur_image(image)\n",
        "                    blur_inputs = self.processor(prompt, images=blurred_image, return_tensors=\"pt\").to(self.model.device)\n",
        "                    blur_outputs = self.model(input_ids=current_ids,\n",
        "                                            pixel_values=blur_inputs[\"pixel_values\"],\n",
        "                                            output_hidden_states=True)\n",
        "\n",
        "\n",
        "                    vcd_logits = outputs.logits + self.alpha * (outputs.logits - blur_outputs.logits)\n",
        "                else:\n",
        "                    #  use ASD-steered logits\n",
        "                    vcd_logits = outputs.logits\n",
        "\n",
        "                # get final logits\n",
        "                final_logits = self.apply_asd_contrast_decoding_fixed(vcd_logits)\n",
        "\n",
        "                # Sample next token from the modified logits\n",
        "                next_token_logits = final_logits[:, -1, :]  # Get last position logits\n",
        "                next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(next_token_probs, 1)\n",
        "\n",
        "                # Check for end of sequence\n",
        "                if next_token.item() == self.processor.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "                generated_tokens.append(next_token.item())\n",
        "\n",
        "                # Update current_ids for next iteration\n",
        "                current_ids = torch.cat([current_ids, next_token], dim=1)\n",
        "\n",
        "        # Decode the generated tokens to text\n",
        "        full_sequence = current_ids[0]\n",
        "        generated_text = self.processor.tokenizer.decode(full_sequence, skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the generated part\n",
        "        prompt_text = self.processor.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
        "        if generated_text.startswith(prompt_text):\n",
        "            generated_text = generated_text[len(prompt_text):].strip()\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    finally:\n",
        "\n",
        "        self.remove_hook()\n",
        "\n",
        "def apply_asd_contrast_decoding_fixed(self, original_logits):\n",
        "    \"\"\"Fixed ASD contrast decoding that actually works\"\"\"\n",
        "    if not hasattr(self, '_positive_output') or not hasattr(self, '_negative_output'):\n",
        "        return original_logits\n",
        "\n",
        "\n",
        "    # Use the difference in steering strengths to create contrast\n",
        "    steering_contrast = self.lambda_negative - self.lambda_positive  # 0.4 - 0.2 = 0.2\n",
        "\n",
        "\n",
        "    contrast_factor = 1.0 + abs(steering_contrast)\n",
        "    enhanced_logits = original_logits * contrast_factor\n",
        "\n",
        "    return enhanced_logits\n",
        "\n",
        "# Add these methods to your VCDSteeringModel class\n",
        "VCDSteeringModel.generate_with_vcd_and_steering_fixed = generate_with_vcd_and_steering_fixed\n",
        "VCDSteeringModel.apply_asd_contrast_decoding_fixed = apply_asd_contrast_decoding_fixed\n",
        "\n"
      ],
      "metadata": {
        "id": "jwLoC7c2rvG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluations"
      ],
      "metadata": {
        "id": "GuOSX_loKaT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "detector = HallucinationDetector()\n",
        "model = VCDSteeringModel()\n",
        "\n",
        "print(\"🔁 Step 1: Collecting steering vector from 1000 COCO samples\")\n",
        "coco = load_dataset(\"mscoco\", split=\"validation[:1000]\")\n",
        "\n",
        "truth_vectors = []\n",
        "halluc_vectors = []\n",
        "initial_scores = []\n",
        "\n",
        "# collect representations using baseline generation\n",
        "for ex in tqdm(coco, desc=\"Collecting representations\"):\n",
        "    image = Image.open(ex[\"image\"]).convert(\"RGB\")\n",
        "    captions = ex[\"captions\"] if isinstance(ex[\"captions\"], list) else [ex[\"caption\"]]\n",
        "    prompt = \"What is happening in this image?\"\n",
        "\n",
        "    # Generate with VCD only\n",
        "    gen_text = model.generate_with_vcd_and_steering_fixed(image, prompt)\n",
        "    score = detector.hallucination_score(gen_text, captions)\n",
        "    initial_scores.append(score)\n",
        "\n",
        "    # Get representation for steering vector computation\n",
        "    vec = model.get_embedding(image, prompt)\n",
        "    if score <= 0.3:  # truthful examples\n",
        "        truth_vectors.append(vec)\n",
        "    elif score >= 0.7:  # hallucinated examples\n",
        "        halluc_vectors.append(vec)\n",
        "\n",
        "print(f\"Collected {len(truth_vectors)} truth vectors and {len(halluc_vectors)} hallucination vectors\")\n",
        "\n",
        "# Build the steering vectors\n",
        "if len(truth_vectors) > 0 and len(halluc_vectors) > 0:\n",
        "    model.compute_steering_vector(truth_vectors, halluc_vectors)\n",
        "    print(\" Steering vectors computed successfully\")\n",
        "\n",
        "\n",
        "#evaluate with steering applied\n",
        "final_scores = []\n",
        "print(\"\\ Step 2: Running final generation with VCD + ASD Steering\")\n",
        "\n",
        "for ex in tqdm(coco, desc=\"Evaluating with steering\"):\n",
        "    image = Image.open(ex[\"image\"]).convert(\"RGB\")\n",
        "    captions = ex[\"captions\"] if isinstance(ex[\"captions\"], list) else [ex[\"caption\"]]\n",
        "    prompt = \"What is happening in this image?\"\n",
        "\n",
        "    # Generate with VCD + ASD steering\n",
        "    gen_text = model.generate_with_vcd_and_steering_fixed(image, prompt)\n",
        "    score = detector.hallucination_score(gen_text, captions)\n",
        "    final_scores.append(score)\n",
        "\n",
        "\n",
        "before = sum(initial_scores) / len(initial_scores)\n",
        "after = sum(final_scores) / len(final_scores)\n",
        "\n",
        "print(f\"\\n RESULTS:\")\n",
        "print(f\"\\n Hallucination Rate After Steering: {after:.3f}\")\n",
        "print(f\" Accuracy After: {(1 - after):.3f} ({(1 - after)*100:.1f}%)\")\n",
        "print(f\"\\n Improvement: {((1 - after) - (1 - before))*100:.1f} percentage points\")\n",
        "\n",
        "print(f\"Hallucination reduction: {((before - after) / before)*100:.1f}%\")"
      ],
      "metadata": {
        "id": "p1qE7PXLKbl-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}