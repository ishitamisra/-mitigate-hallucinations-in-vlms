# Configuration file for LLaVA VCD+ASD model

# Model settings
model:
  name: "liuhaotian/llava-v1.5-13b"  # HuggingFace model name
  device: "auto"  # Device placement strategy
  trust_remote_code: true
  torch_dtype: "float16"

# VCD (Visual Contrastive Decoding) parameters
vcd:
  alpha: 0.1  # Weight for contrastive decoding
  blur_intensity: 0.5  # Gaussian blur intensity for degraded images

# ASD (Activation Steering Decoding) parameters
asd:
  target_layer: 16  # Layer index for activation steering
  lambda_positive: 0.2  # Positive steering strength (toward truth)
  lambda_negative: 0.4  # Negative steering strength (away from hallucination)
  contrast_alpha: 1.0  # Alpha parameter for contrast decoding

# Training settings
training:
  dataset_name: "ydshieh/coco_dataset_script"
  split: "validation"
  num_samples: 1000  # Number of samples for steering vector computation
  truth_threshold: 0.3  # Max hallucination score for truth examples
  halluc_threshold: 0.7  # Min hallucination score for hallucination examples
  
  # Hyperparameter optimization
  optimize_hyperparams: true
  num_trials: 20
  param_ranges:
    vcd_alpha: [0.05, 0.1, 0.15, 0.2, 0.25]
    lambda_positive: [0.1, 0.2, 0.3, 0.4]
    lambda_negative: [0.2, 0.4, 0.6, 0.8]
    blur_intensity: [0.3, 0.5, 0.7, 1.0]

# Evaluation settings
evaluation:
  dataset_name: "ydshieh/coco_dataset_script"
  split: "validation"
  num_samples: 500
  prompts:
    - "Describe this image in detail."
    - "What objects can you see in this image?"
    - "What is happening in this image?"
  
  # Generation parameters
  max_new_tokens: 100
  temperature: 0.7
  do_sample: true

# Output settings
output:
  base_dir: "outputs"
  steering_vectors_filename: "steering_vectors.pt"
  steering_data_filename: "steering_data.json"
  evaluation_dir: "evaluation_results"

# Logging settings
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"